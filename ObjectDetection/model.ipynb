{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # Для обучения нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "architecture = [\n",
    "    (7, 64, 2, 3),\n",
    "    \"M\",\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1),\n",
    "    (1, 256, 1, 0),\n",
    "    (3, 512, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0),\n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\",\n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 1, 1)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Yolov1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(Yolov1, self).__init__()\n",
    "        self.architecture = architecture\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * split_size * split_size, 512),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(512, split_size*split_size*(num_classes+num_boxes*5))\n",
    "        )\n",
    "\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels=x[1],\n",
    "                        kernel_size=x[0],\n",
    "                        stride = x[2],\n",
    "                        padding = x[3]\n",
    "                    )\n",
    "                )\n",
    "                in_channels = x[1]\n",
    "            elif type(x) == str:\n",
    "                layers.append(\n",
    "                    nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "                )\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        nn.Conv2d(\n",
    "                            in_channels,\n",
    "                            out_channels=conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3]\n",
    "                        ),\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=conv1[1],\n",
    "                            out_channels=conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3]\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        return self.fcs(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lamda_noobj = 0.5\n",
    "        self.lamda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B*5)\n",
    "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        bestbox = bestbox.unsqueeze(3)\n",
    "        exists_box = target[..., 20].unsqueeze(3)\n",
    "\n",
    "        ### box coordinates\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., 26:30] + (1-bestbox)* predictions[..., 21:25]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., 21:25]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4])* torch.sqrt(torch.abs(box_predictions[..., 2:4]) + 1e-6)\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions.detach(), end_dim=-2),\n",
    "                            torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "\n",
    "        ### object loss\n",
    "\n",
    "        pred_box = bestbox * predictions[..., 25:26] + (1-bestbox) * predictions[..., 20:21]\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box.detach()*pred_box.detach()),\n",
    "            torch.flatten(exists_box.detach()*target[..., 20:21].detach())\n",
    "        )\n",
    "        ### no object loss\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[..., 20:21].detach(), start_dim=1),\n",
    "            torch.flatten((1-exists_box)* target[..., 20:21].detach(), start_dim=1)\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[..., 25:26].detach(), start_dim=1),\n",
    "            torch.flatten((1-exists_box)* target[..., 20:21].detach(), start_dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        ###class loss\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box.detach() * predictions[..., :20].detach(), end_dim=-2),\n",
    "            torch.flatten(exists_box.detach() * target[..., :20].detach(), end_dim=-2)\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = (\n",
    "            self.lamda_coord*box_loss + self.lamda_noobj*no_object_loss + object_loss + class_loss\n",
    "        )\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class YoloLoss2(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=20):\n",
    "        super(YoloLoss2, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lamda_noobj = 0.5\n",
    "        self.lamda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B*5)\n",
    "\n",
    "        left_target_box = self.C+1\n",
    "        right_target_box = self.C+1+4\n",
    "\n",
    "        iou_b1 = intersection_over_union(predictions[..., left_target_box:right_target_box], target[..., left_target_box:right_target_box])\n",
    "        iou_b2 = intersection_over_union(predictions[..., left_target_box+5:right_target_box+5], target[..., left_target_box:right_target_box])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
    "        bestbox = bestbox.unsqueeze(3)\n",
    "        exists_box = target[..., self.C].unsqueeze(3)\n",
    "\n",
    "        ### box coordinates\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                bestbox * predictions[..., left_target_box+5:right_target_box+5] + (1-bestbox)* predictions[..., left_target_box:right_target_box]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        box_targets = exists_box * target[..., left_target_box:right_target_box]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4])* torch.sqrt(torch.abs(box_predictions[..., 2:4]) + 1e-6)\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(torch.flatten(box_predictions.detach(), end_dim=-2),\n",
    "                            torch.flatten(box_targets, end_dim=-2))\n",
    "\n",
    "\n",
    "        ### object loss\n",
    "\n",
    "        pred_box = bestbox * predictions[..., right_target_box:right_target_box+1] + (1-bestbox) * predictions[..., self.C:left_target_box]\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box.detach()*pred_box),\n",
    "            torch.flatten(exists_box.detach()*target[..., self.C:left_target_box])\n",
    "        )\n",
    "        ### no object loss\n",
    "\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[..., self.C:left_target_box], start_dim=1),\n",
    "            torch.flatten((1-exists_box)* target[..., self.C:left_target_box], start_dim=1)\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1-exists_box)*predictions[..., right_target_box:right_target_box+1], start_dim=1),\n",
    "            torch.flatten((1-exists_box)* target[..., self.C:left_target_box], start_dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        ###class loss\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :self.C], end_dim=-2),\n",
    "            torch.flatten(exists_box * target[..., :self.C], end_dim=-2)\n",
    "        )\n",
    "\n",
    "\n",
    "        loss = (\n",
    "            self.lamda_coord*box_loss + self.lamda_noobj*no_object_loss + object_loss + class_loss\n",
    "        )\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intersection_over_union' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m l \u001B[38;5;241m=\u001B[39m \u001B[43mYoloLoss2\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m12\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequires_grad_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mones\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m7\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequires_grad_\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m l\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [16]\u001B[0m, in \u001B[0;36mYoloLoss2.forward\u001B[1;34m(self, predictions, target)\u001B[0m\n\u001B[0;32m     14\u001B[0m left_target_box \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     15\u001B[0m right_target_box \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m4\u001B[39m\n\u001B[1;32m---> 17\u001B[0m iou_b1 \u001B[38;5;241m=\u001B[39m \u001B[43mintersection_over_union\u001B[49m(predictions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, left_target_box:right_target_box], target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, left_target_box:right_target_box])\n\u001B[0;32m     18\u001B[0m iou_b2 \u001B[38;5;241m=\u001B[39m intersection_over_union(predictions[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, left_target_box\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m5\u001B[39m:right_target_box\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m5\u001B[39m], target[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, left_target_box:right_target_box])\n\u001B[0;32m     19\u001B[0m ious \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([iou_b1\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), iou_b2\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'intersection_over_union' is not defined"
     ]
    }
   ],
   "source": [
    "l = YoloLoss2(7,2,2)(torch.ones(1, 7, 7, 12).requires_grad_(), torch.ones(1, 7, 7, 7).requires_grad_())\n",
    "l.backward()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'D:\\mlp\\MachineLearning\\venv\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from  torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class VOCDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_file,\n",
    "                 S = 7,\n",
    "                 B = 2,\n",
    "                 C = 20):\n",
    "        super(VOCDataset, self).__init__()\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((448, 448))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.annotations.iloc[item, 0]\n",
    "        label_path = self.annotations.iloc[item, 1]\n",
    "        boxes = []\n",
    "        with open(label_path) as f:\n",
    "            file = f.read().split('\\n')[:-1]\n",
    "        file = [list(map(float, x.split(' '))) for x in file]\n",
    "        for i in file:\n",
    "            boxes.append(i)\n",
    "        image = Image.open(img_path)\n",
    "        image = self.transforms(image)\n",
    "\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C+5))\n",
    "\n",
    "        left_target_box = self.C+1\n",
    "        right_target_box = self.C+1+4\n",
    "\n",
    "        for box in boxes:\n",
    "            class_label, x,  y, width, height = box\n",
    "            class_label = int(class_label)\n",
    "            i, j  = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "            width_cell, height_cell = width * self.S, height*self.S\n",
    "\n",
    "            if label_matrix[i, j, self.C] == 0:\n",
    "                label_matrix[i, j, self.C] = 1\n",
    "                box_coordinates = torch.tensor([\n",
    "                    x_cell, y_cell, width_cell, height_cell\n",
    "                ])\n",
    "                label_matrix[i, j, left_target_box:right_target_box] = box_coordinates\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return(image, label_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "qq = VOCDataset('train.csv', 7,2,2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7, 7, 7])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qq.__getitem__(2)[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from IntersectionOverUnion import intersection_over_union\n",
    "from MeanAveragePrecision import mean_average_precision\n",
    "from NonMaximumSupression import non_maximum_suppression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "lr = 2e-5\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device='cpu'\n",
    "batch_size = 4\n",
    "epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv')\n",
    "test_csv = pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "model = Yolov1(3, split_size=7, num_boxes=2, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_function = YoloLoss2(7, 2, 2)\n",
    "scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5, patience=2, threshold=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "train_dataset = VOCDataset('train.csv',7,2,2)\n",
    "test_dataset = VOCDataset('test.csv',7,2,2)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "all_train_losses = []\n",
    "all_test_losses = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Для отчистки графика во время обучения НС\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                num_epochs,\n",
    "                loss_function,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                train_dataloader,\n",
    "                test_dataloader):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        model.train(True)\n",
    "        print(f'epoch_number is {epoch}. Train')\n",
    "        for (X, y) in tqdm(train_dataloader):\n",
    "            model.zero_grad()\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = model.forward(X)\n",
    "            loss = loss_function(preds.requires_grad_(), y.requires_grad_())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.detach().item()/ (len(train_dataloader))\n",
    "            with torch.no_grad():\n",
    "                torch.cuda.empty_cache()\n",
    "        model.train(False)\n",
    "        model.eval()\n",
    "        print(f'epoch_number is {epoch}. Test')\n",
    "        for (X, y) in tqdm(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                preds = model.forward(X)\n",
    "                loss = loss_function(preds, y)\n",
    "                test_loss += loss.detach().item() / (len(test_dataloader))\n",
    "        scheduler.step(test_loss)\n",
    "        clear_output()\n",
    "        all_train_losses.append(train_loss)\n",
    "        all_test_losses.append(test_loss)\n",
    "        print('loss train', train_loss)\n",
    "        print('loss test', test_loss)\n",
    "        plt.figure(figsize = (10, 6))\n",
    "        plt.plot(all_train_losses, label = 'Train loss', color = 'blue')\n",
    "        plt.plot(all_test_losses, label = 'Val loss', color = 'orange')\n",
    "        plt.legend()\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss train 259.5988798196605\n",
      "loss test 231.58856430053712\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_number is 24. Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 11/173 [00:02<00:42,  3.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [30]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [29]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, num_epochs, loss_function, optimizer, scheduler, train_dataloader, test_dataloader)\u001B[0m\n\u001B[0;32m     11\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepoch_number is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Train\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (X, y) \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dataloader):\n\u001B[0;32m     14\u001B[0m     model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     15\u001B[0m     X \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\tqdm\\std.py:1180\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1177\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1180\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1181\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1182\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1183\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[1;32mIn [19]\u001B[0m, in \u001B[0;36mVOCDataset.__getitem__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m     30\u001B[0m     boxes\u001B[38;5;241m.\u001B[39mappend(i)\n\u001B[0;32m     31\u001B[0m image \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(img_path)\n\u001B[1;32m---> 32\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransforms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m label_matrix \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mS, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mS, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m5\u001B[39m))\n\u001B[0;32m     36\u001B[0m left_target_box \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mC\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:61\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 61\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:98\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 98\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:141\u001B[0m, in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;66;03m# handle PIL Image\u001B[39;00m\n\u001B[0;32m    139\u001B[0m mode_to_nptype \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mint32, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mI;16\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mint16, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m'\u001B[39m: np\u001B[38;5;241m.\u001B[39mfloat32}\n\u001B[0;32m    140\u001B[0m img \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\n\u001B[1;32m--> 141\u001B[0m     \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_to_nptype\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m )\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    145\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\PIL\\Image.py:675\u001B[0m, in \u001B[0;36mImage.__array__\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m    673\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtobytes(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    674\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 675\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtobytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ArrayData(new), dtype)\n",
      "File \u001B[1;32md:\\mlp\\machinelearning\\venv\\lib\\site-packages\\PIL\\Image.py:735\u001B[0m, in \u001B[0;36mImage.tobytes\u001B[1;34m(self, encoder_name, *args)\u001B[0m\n\u001B[0;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m s \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    733\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoder error \u001B[39m\u001B[38;5;132;01m{\u001B[39;00ms\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in tobytes\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 735\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, epochs, loss_function, optimizer, scheduler, train_dataloader, test_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'yolo.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = model(test_dataset.__getitem__(0)[0].unsqueeze(0).cuda())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "res = res.reshape(-1, 7, 7, 2 + 2*5).tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "final = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "for picture in res:\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            width1, height1 = picture[i][j][5]*7, picture[i][j][6]*7\n",
    "            width2, height2 = picture[i][j][10]*7, picture[i][j][11]*7\n",
    "\n",
    "            x1 = (picture[i][j][3]+j)/7\n",
    "            y1 = (picture[i][j][4]+i)/7\n",
    "\n",
    "            x2 = (picture[i][j][8]+j)/7\n",
    "            y2 = (picture[i][j][9]+i)/7\n",
    "\n",
    "            p1 = picture[i][j][2]\n",
    "            p2 = picture[i][j][7]\n",
    "\n",
    "            c = np.argmax([picture[i][j][0], picture[i][j][1]])\n",
    "            final.append([c, p1, x1, y1, width1, height1])\n",
    "            final.append([c, p2, x2, y2, width2, height2])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "50"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.array(final)[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "bbb = np.abs(np.array(final)[:, 2:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "img = Image.open('dataset/dataset/images/test/TB2R28UvUOWBKNjSZKzXXXfWFXa_!!1116877752.jpg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(800, 800)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "bbb = bbb*800"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35,\n",
    "        color=TEXT_COLOR,\n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "import cv2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[1.0000, 1.0000, 1.0000,  ..., 0.7647, 0.7647, 0.7647],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7647, 0.7647, 0.7647],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7647, 0.7647, 0.7647],\n          ...,\n          [1.0000, 1.0000, 1.0000,  ..., 0.3195, 0.6049, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.3206, 0.6623, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.3190, 0.7045, 0.9985]],\n\n         [[1.0000, 1.0000, 1.0000,  ..., 0.7608, 0.7608, 0.7608],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7608, 0.7608, 0.7608],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7608, 0.7608, 0.7608],\n          ...,\n          [1.0000, 1.0000, 1.0000,  ..., 0.2724, 0.5772, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.2735, 0.6398, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.2721, 0.6840, 1.0000]],\n\n         [[1.0000, 1.0000, 1.0000,  ..., 0.7451, 0.7451, 0.7451],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7451, 0.7451, 0.7451],\n          [1.0000, 1.0000, 1.0000,  ..., 0.7451, 0.7451, 0.7451],\n          ...,\n          [1.0000, 1.0000, 1.0000,  ..., 0.2724, 0.5772, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.2735, 0.6372, 1.0000],\n          [1.0000, 1.0000, 1.0000,  ..., 0.2720, 0.6804, 1.0000]]]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.__getitem__(0)[0].unsqueeze(0).cuda()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                   img  \\\n0    dataset/dataset/images/test/TB2R28UvUOWBKNjSZK...   \n1    dataset/dataset/images/test/US-expands-coronav...   \n2    dataset/dataset/images/test/Pakistan-China-cor...   \n3    dataset/dataset/images/test/RZXE24HFK5GSVKVSN5...   \n4         dataset/dataset/images/test/TASS38012699.jpg   \n..                                                 ...   \n114            dataset/dataset/images/test/sgssgg.jpeg   \n115  dataset/dataset/images/test/upload-productImg-...   \n116  dataset/dataset/images/test/TELEMMGLPICT000222...   \n117            dataset/dataset/images/test/stsciRq.png   \n118  dataset/dataset/images/test/usa-people-waiting...   \n\n                                                 label  \n0    dataset/dataset/images/test/TB2R28UvUOWBKNjSZK...  \n1    dataset/dataset/images/test/US-expands-coronav...  \n2    dataset/dataset/images/test/Pakistan-China-cor...  \n3    dataset/dataset/images/test/RZXE24HFK5GSVKVSN5...  \n4         dataset/dataset/images/test/TASS38012699.txt  \n..                                                 ...  \n114             dataset/dataset/images/test/sgssgg.txt  \n115  dataset/dataset/images/test/upload-productImg-...  \n116  dataset/dataset/images/test/TELEMMGLPICT000222...  \n117            dataset/dataset/images/test/stsciRq.txt  \n118  dataset/dataset/images/test/usa-people-waiting...  \n\n[119 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dataset/dataset/images/test/TB2R28UvUOWBKNjSZK...</td>\n      <td>dataset/dataset/images/test/TB2R28UvUOWBKNjSZK...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dataset/dataset/images/test/US-expands-coronav...</td>\n      <td>dataset/dataset/images/test/US-expands-coronav...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>dataset/dataset/images/test/Pakistan-China-cor...</td>\n      <td>dataset/dataset/images/test/Pakistan-China-cor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dataset/dataset/images/test/RZXE24HFK5GSVKVSN5...</td>\n      <td>dataset/dataset/images/test/RZXE24HFK5GSVKVSN5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dataset/dataset/images/test/TASS38012699.jpg</td>\n      <td>dataset/dataset/images/test/TASS38012699.txt</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>dataset/dataset/images/test/sgssgg.jpeg</td>\n      <td>dataset/dataset/images/test/sgssgg.txt</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>dataset/dataset/images/test/upload-productImg-...</td>\n      <td>dataset/dataset/images/test/upload-productImg-...</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>dataset/dataset/images/test/TELEMMGLPICT000222...</td>\n      <td>dataset/dataset/images/test/TELEMMGLPICT000222...</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>dataset/dataset/images/test/stsciRq.png</td>\n      <td>dataset/dataset/images/test/stsciRq.txt</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>dataset/dataset/images/test/usa-people-waiting...</td>\n      <td>dataset/dataset/images/test/usa-people-waiting...</td>\n    </tr>\n  </tbody>\n</table>\n<p>119 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1cb64eac880>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.autograd.set_detect_anomaly(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 588])"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0212,  0.0301,  0.0100,  ...,  0.0279,  0.0381,  0.0253],\n        [-0.0212,  0.0301,  0.0100,  ...,  0.0279,  0.0381,  0.0253]],\n       device='cuda:0')"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 7, 1]) torch.Size([2, 7, 7, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(2296.7966)"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YoloLoss2(7,2,2).forward(ex.cpu(), torch.ones(2, 7, 7, 7))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]]],\n\n\n        [[[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]],\n\n         [[0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.],\n          [0., 0., 0., 0.]]]])"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 7, 7, 1)* torch.ones(2,7,7,4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1470])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 7, 7, 25])"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 7, 7, 25).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1470])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         ...,\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]]],\n\n\n        [[[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         ...,\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]],\n\n         [[1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          ...,\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.],\n          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 7, 7, 25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n          28,  29],\n        [ 30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,\n          44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  57,\n          58,  59],\n        [ 60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n          74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,\n          88,  89],\n        [ 90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n         104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n         118, 119]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(range(120)).reshape(4, 30)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "q = torch.cat([intersection_over_union(a[:, 21:25], a[:, 21:25]).unsqueeze(0), intersection_over_union(a[:, 21:25], a[:, 21:25]).unsqueeze(0)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.max(\nvalues=tensor([1., 1., 1., 1.]),\nindices=tensor([0, 0, 0, 0]))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(q, dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([20, 45, 70, 95])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(100)).reshape(4, 25)[:, 20]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}